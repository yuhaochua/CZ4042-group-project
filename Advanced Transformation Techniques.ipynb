{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b62977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as scp\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import functional as F\n",
    "import math\n",
    "from dataset import train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code in this notebook is referenced from https://pytorch.org/vision/master/auto_examples/transforms/plot_cutmix_mixup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee778af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fb33625",
   "metadata": {},
   "source": [
    "# CutMix\n",
    "\n",
    "> **Cut-and-Paste Data Augmentation**: CutMix combines two or more images by cutting a rectangular portion from one image and pasting it onto another. The pixel values of the pasted region are a combination of the original image and the selected portion from another image.\n",
    "<br>\n",
    ">\n",
    "> **Label Mixing**: The labels of the pasted region are also combined based on the area. This means that if 60% of the region comes from image A and 40% from image B, the label for that region is mixed accordingly.\n",
    "<br>\n",
    ">\n",
    "> **Smooth Regularization**: CutMix acts as a regularization technique to prevent overfitting. It encourages the model to make predictions on the mixed regions, which can lead to improved generalization.\n",
    "<br>\n",
    ">\n",
    "> **Benefits**: CutMix can improve model robustness, make the model less sensitive to input perturbations, and lead to better generalization. It is especially useful when training on smaller datasets.\n",
    "<br>\n",
    ">\n",
    "# MixUp\n",
    "\n",
    "> **Linear Interpolation**: MixUp operates by linearly interpolating between pairs of input samples. Given two input samples and their corresponding labels, MixUp creates new training examples by taking a weighted sum of the two samples. The labels are also linearly interpolated.\n",
    "<br>\n",
    ">\n",
    "> **Smooth Labeling**: MixUp effectively \"softens\" the labels by blending them. For example, if you mix two images with labels \"cat\" and \"dog\" with a mixing factor of 0.7, the new image's label will be a soft label with 70% \"cat\" and 30% \"dog.\"\n",
    "<br>\n",
    ">\n",
    "> **Benefits**: MixUp encourages the model to make predictions that are linear combinations of the original data points. It helps the model learn a more generalized decision boundary and reduce the risk of overfitting. It also aids in handling class imbalance.\n",
    "<br>\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89db27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(data, target, alpha=1.0):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + data[indices] * (1 - lam)\n",
    "    target = target * lam + target[indices] * (1 - lam)\n",
    "    return data, target\n",
    "\n",
    "def mixup(data, target, alpha=1.0):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    data = data * lam + data[indices] * (1 - lam)\n",
    "    target = target * lam + target[indices] * (1 - lam)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda7b3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES=102\n",
    "cutmix = v2.CutMix(num_classes=NUM_CLASSES)\n",
    "mixup = v2.MixUp(num_classes=NUM_CLASSES)\n",
    "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, device, cutmix_prob=0.5, mixup_prob=0.5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs, targets = cutmix_or_mixup(inputs, targets)\n",
    "        _, targets = targets.max(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(train_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06ae43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import eval\n",
    "from model import mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bda3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import EarlyStopper\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_EPOCH = 100\n",
    "NUM_CLASSES = 5\n",
    "EARLY_STOP_THRESHOLD = 3\n",
    "early_stopper = EarlyStopper(patience=EARLY_STOP_THRESHOLD)\n",
    "\n",
    "# HYPERPARAMS TO TUNE\n",
    "BATCH_SIZE = 128\n",
    "EARLY_STOP_THRESHOLD = 3\n",
    "LR = 0.001\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "model,optimizer,criterion = mobilenet()\n",
    "model.to(DEVICE)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "best_acc = 0\n",
    "early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc54b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: (4.790738523006439, 0.008823529411764706), Val Accuracy: 0.09411764705882353\n",
      "Epoch 2, Train Loss: (4.339960753917694, 0.07450980392156863), Val Accuracy: 0.24803921568627452\n",
      "Epoch 3, Train Loss: (4.066323310136795, 0.15588235294117647), Val Accuracy: 0.46568627450980393\n",
      "Epoch 4, Train Loss: (3.719339430332184, 0.27058823529411763), Val Accuracy: 0.5696078431372549\n",
      "Epoch 5, Train Loss: (3.5453310310840607, 0.28921568627450983), Val Accuracy: 0.6460784313725491\n",
      "Epoch 6, Train Loss: (3.2798059284687042, 0.39705882352941174), Val Accuracy: 0.6647058823529411\n",
      "Epoch 7, Train Loss: (3.233849823474884, 0.4088235294117647), Val Accuracy: 0.6627450980392157\n",
      "Epoch 8, Train Loss: (3.1141715943813324, 0.43333333333333335), Val Accuracy: 0.6892156862745098\n",
      "Epoch 9, Train Loss: (3.0263059735298157, 0.4627450980392157), Val Accuracy: 0.7186274509803922\n",
      "Epoch 10, Train Loss: (2.8191276490688324, 0.5196078431372549), Val Accuracy: 0.711764705882353\n",
      "Epoch 11, Train Loss: (3.0044323801994324, 0.4519607843137255), Val Accuracy: 0.7205882352941176\n",
      "Epoch 12, Train Loss: (2.937171459197998, 0.4676470588235294), Val Accuracy: 0.7245098039215686\n",
      "Epoch 13, Train Loss: (2.577829748392105, 0.5450980392156862), Val Accuracy: 0.7421568627450981\n",
      "Epoch 14, Train Loss: (3.1700457334518433, 0.3686274509803922), Val Accuracy: 0.7313725490196078\n",
      "Epoch 15, Train Loss: (2.6098624169826508, 0.5490196078431373), Val Accuracy: 0.7470588235294118\n",
      "Epoch 16, Train Loss: (2.7084369361400604, 0.538235294117647), Val Accuracy: 0.7578431372549019\n",
      "Epoch 17, Train Loss: (2.5974044501781464, 0.5480392156862746), Val Accuracy: 0.7617647058823529\n",
      "Epoch 18, Train Loss: (2.7369796335697174, 0.5068627450980392), Val Accuracy: 0.7598039215686274\n",
      "Epoch 19, Train Loss: (2.653329223394394, 0.5225490196078432), Val Accuracy: 0.7666666666666667\n",
      "Epoch 20, Train Loss: (2.5434905737638474, 0.5450980392156862), Val Accuracy: 0.7676470588235295\n",
      "Epoch 21, Train Loss: (2.7485974729061127, 0.5), Val Accuracy: 0.7519607843137255\n",
      "Epoch 22, Train Loss: (2.624886304140091, 0.5333333333333333), Val Accuracy: 0.7666666666666667\n",
      "Epoch 23, Train Loss: (2.70871502161026, 0.5107843137254902), Val Accuracy: 0.765686274509804\n",
      "Epoch 24, Train Loss: (2.909872591495514, 0.44509803921568625), Val Accuracy: 0.7450980392156863\n",
      "Epoch 25, Train Loss: (2.5299984961748123, 0.5598039215686275), Val Accuracy: 0.7529411764705882\n",
      "Epoch 26, Train Loss: (2.5096916556358337, 0.5892156862745098), Val Accuracy: 0.7696078431372549\n",
      "Epoch 27, Train Loss: (2.543144002556801, 0.5392156862745098), Val Accuracy: 0.7735294117647059\n",
      "Epoch 28, Train Loss: (2.561808705329895, 0.538235294117647), Val Accuracy: 0.7764705882352941\n",
      "Epoch 29, Train Loss: (2.758589133620262, 0.49019607843137253), Val Accuracy: 0.7735294117647059\n",
      "Epoch 30, Train Loss: (2.9140281081199646, 0.44019607843137254), Val Accuracy: 0.7637254901960784\n",
      "Early Stopping...\n",
      "Test Accuracy: 0.7176776711660433\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCH+1):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer, DEVICE)\n",
    "    accuracy, val_loss = eval(val_loader, model, criterion, DEVICE)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Val Accuracy: {accuracy}')\n",
    "    if early_stopper.early_stop(val_loss):\n",
    "        print(\"Early Stopping...\")\n",
    "        break\n",
    "    scheduler.step()\n",
    "test_accuracy, _ = eval(test_loader, model, criterion, DEVICE)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba9bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
