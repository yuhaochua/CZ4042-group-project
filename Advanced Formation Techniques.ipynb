{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b62977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as scp\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee778af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = v2.Compose([\n",
    "#     v2.PILToTensor(),\n",
    "#     v2.RandomRotation(30),\n",
    "#     v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "#     v2.RandomHorizontalFlip(p=0.5),\n",
    "#     v2.ToDtype(torch.float32, scale=True),  # to float32 in [0, 1]\n",
    "#     v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "# ])\n",
    "\n",
    "# testval_transform = v2.Compose([\n",
    "#     v2.PILToTensor(),\n",
    "#     v2.Resize(256),\n",
    "#     v2.CenterCrop(224),\n",
    "#     v2.ToDtype(torch.float32, scale=True),  # to float32 in [0, 1]\n",
    "#     v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "# ])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(30), # random rotation of images\n",
    "    transforms.RandomResizedCrop(224), # sample random 224x224 patch of images\n",
    "    transforms.RandomHorizontalFlip(), # random horizontal flip of images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                        [0.229, 0.224, 0.225])\n",
    "    # transforms.Normalize([0.51796, 0.41106, 0.32971], \n",
    "    #                     [0.29697, 0.24955, 0.28531])\n",
    "    ])\n",
    "\n",
    "testval_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                        [0.229, 0.224, 0.225])\n",
    "    # transforms.Normalize([0.51796, 0.41106, 0.32971], \n",
    "    #                     [0.29697, 0.24955, 0.28531])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.Flowers102(root='./data', split='train', download=True, transform=train_transform)\n",
    "val_dataset = torchvision.datasets.Flowers102(root='./data', split='val', download=True, transform=testval_transform)\n",
    "test_dataset = torchvision.datasets.Flowers102(root='./data', split='test', download=True, transform=testval_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb33625",
   "metadata": {},
   "source": [
    "# CutMix\n",
    "\n",
    "> **Cut-and-Paste Data Augmentation**: CutMix combines two or more images by cutting a rectangular portion from one image and pasting it onto another. The pixel values of the pasted region are a combination of the original image and the selected portion from another image.\n",
    "<br>\n",
    ">\n",
    "> **Label Mixing**: The labels of the pasted region are also combined based on the area. This means that if 60% of the region comes from image A and 40% from image B, the label for that region is mixed accordingly.\n",
    "<br>\n",
    ">\n",
    "> **Smooth Regularization**: CutMix acts as a regularization technique to prevent overfitting. It encourages the model to make predictions on the mixed regions, which can lead to improved generalization.\n",
    "<br>\n",
    ">\n",
    "> **Benefits**: CutMix can improve model robustness, make the model less sensitive to input perturbations, and lead to better generalization. It is especially useful when training on smaller datasets.\n",
    "<br>\n",
    ">\n",
    "# MixUp\n",
    "\n",
    "> **Linear Interpolation**: MixUp operates by linearly interpolating between pairs of input samples. Given two input samples and their corresponding labels, MixUp creates new training examples by taking a weighted sum of the two samples. The labels are also linearly interpolated.\n",
    "<br>\n",
    ">\n",
    "> **Smooth Labeling**: MixUp effectively \"softens\" the labels by blending them. For example, if you mix two images with labels \"cat\" and \"dog\" with a mixing factor of 0.7, the new image's label will be a soft label with 70% \"cat\" and 30% \"dog.\"\n",
    "<br>\n",
    ">\n",
    "> **Benefits**: MixUp encourages the model to make predictions that are linear combinations of the original data points. It helps the model learn a more generalized decision boundary and reduce the risk of overfitting. It also aids in handling class imbalance.\n",
    "<br>\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89db27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(data, target, alpha=1.0):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + data[indices] * (1 - lam)\n",
    "    target = target * lam + target[indices] * (1 - lam)\n",
    "    return data, target\n",
    "\n",
    "def mixup(data, target, alpha=1.0):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    data = data * lam + data[indices] * (1 - lam)\n",
    "    target = target * lam + target[indices] * (1 - lam)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda7b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(train_loader, model, criterion, optimizer, device, cutmix_prob=0.5, mixup_prob=0.5):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "#         print(targets)\n",
    "#         print(targets.dtype)\n",
    "#         print(inputs.dtype)\n",
    "        \n",
    "#         if random.random() < cutmix_prob:\n",
    "#             inputs, targets = cutmix(inputs, targets)\n",
    "#         elif random.random() < mixup_prob:\n",
    "#             inputs, targets = mixup(inputs, targets)\n",
    "        \n",
    "#         targets.to(torch.int64)\n",
    "#         print(targets.dtype)\n",
    "#         print(targets)\n",
    "#         print(inputs.dtype)\n",
    "#         # targets = targets.long()\n",
    "#         # inputs = inputs.long()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         print(outputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         total += targets.size(0)\n",
    "#         correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "#     return running_loss / len(train_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES=102\n",
    "cutmix = v2.CutMix(num_classes=NUM_CLASSES)\n",
    "mixup = v2.MixUp(num_classes=NUM_CLASSES)\n",
    "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, device, cutmix_prob=0.5, mixup_prob=0.5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # print(targets)\n",
    "        # print(f\"Before CutMix/MixUp: {inputs.shape = }, {targets.shape = }\")\n",
    "        inputs, targets = cutmix_or_mixup(inputs, targets)\n",
    "        # print(f\"After CutMix/MixUp: {inputs.shape = }, {targets.shape = }\")\n",
    "        # print(targets)\n",
    "        _, targets = targets.max(1)\n",
    "        # print(targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs.shape)\n",
    "        # print(targets.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(train_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06ae43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import eval\n",
    "from model import mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a95637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Calculate the cross-entropy loss with soft labels\n",
    "        loss = -torch.sum(target * torch.log(input + 1e-8)) / input.size(0)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bda3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import EarlyStopper\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_EPOCH = 100\n",
    "NUM_CLASSES = 5\n",
    "EARLY_STOP_THRESHOLD = 3\n",
    "early_stopper = EarlyStopper(patience=EARLY_STOP_THRESHOLD)\n",
    "\n",
    "# HYPERPARAMS TO TUNE\n",
    "NUM_HIDDEN = 128\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 128\n",
    "EARLY_STOP_THRESHOLD = 3\n",
    "LR = 0.001\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "model,optimizer,criterion = mobilenet()\n",
    "model.to(DEVICE)\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "best_acc = 0\n",
    "early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc54b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: (4.820556700229645, 0.00784313725490196), Val Accuracy: 0.08823529411764706\n",
      "Epoch 2, Train Loss: (4.40290379524231, 0.052941176470588235), Val Accuracy: 0.22156862745098038\n",
      "Epoch 3, Train Loss: (4.088704615831375, 0.12745098039215685), Val Accuracy: 0.45784313725490194\n",
      "Epoch 4, Train Loss: (3.685787260532379, 0.28137254901960784), Val Accuracy: 0.5529411764705883\n",
      "Epoch 5, Train Loss: (3.544148772954941, 0.3235294117647059), Val Accuracy: 0.6333333333333333\n",
      "Epoch 6, Train Loss: (3.2715260088443756, 0.3990196078431373), Val Accuracy: 0.6509803921568628\n",
      "Epoch 7, Train Loss: (3.196895331144333, 0.40980392156862744), Val Accuracy: 0.6872549019607843\n",
      "Epoch 8, Train Loss: (3.417306274175644, 0.3235294117647059), Val Accuracy: 0.7098039215686275\n",
      "Epoch 9, Train Loss: (3.0151030719280243, 0.45294117647058824), Val Accuracy: 0.7225490196078431\n",
      "Epoch 10, Train Loss: (3.294542580842972, 0.3607843137254902), Val Accuracy: 0.7147058823529412\n",
      "Epoch 11, Train Loss: (2.951190948486328, 0.46568627450980393), Val Accuracy: 0.7156862745098039\n",
      "Epoch 12, Train Loss: (2.847430109977722, 0.49411764705882355), Val Accuracy: 0.7450980392156863\n",
      "Epoch 13, Train Loss: (2.7859974205493927, 0.49901960784313726), Val Accuracy: 0.7627450980392156\n",
      "Epoch 14, Train Loss: (2.978765517473221, 0.44803921568627453), Val Accuracy: 0.7578431372549019\n",
      "Epoch 15, Train Loss: (3.010046660900116, 0.4294117647058823), Val Accuracy: 0.7549019607843137\n",
      "Epoch 16, Train Loss: (2.5333096385002136, 0.5794117647058824), Val Accuracy: 0.7647058823529411\n",
      "Epoch 17, Train Loss: (2.7352259159088135, 0.5058823529411764), Val Accuracy: 0.7578431372549019\n",
      "Epoch 18, Train Loss: (2.9604079723358154, 0.43529411764705883), Val Accuracy: 0.7490196078431373\n",
      "Epoch 19, Train Loss: (2.7232846915721893, 0.5068627450980392), Val Accuracy: 0.7588235294117647\n",
      "Early Stopping...\n",
      "Test Accuracy: 0.7019027484143763\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCH+1):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer, DEVICE)\n",
    "    accuracy, val_loss = eval(val_loader, model, criterion, DEVICE)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Val Accuracy: {accuracy}')\n",
    "    if early_stopper.early_stop(val_loss):\n",
    "        print(\"Early Stopping...\")\n",
    "        break\n",
    "    scheduler.step()\n",
    "test_accuracy, _ = eval(test_loader, model, criterion, DEVICE)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba9bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
