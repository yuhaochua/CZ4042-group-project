{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5155b6ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io as scp\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from dataset import train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db955532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc8aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(pil_image):    \n",
    "    # -------- Resize with Aspect Ratio maintained--------- #\n",
    "    # First fixing the short axes\n",
    "    if pil_image.size[0] > pil_image.size[1]:\n",
    "        pil_image.thumbnail((10000000, 256))\n",
    "    else:\n",
    "        pil_image.thumbnail((256, 100000000))\n",
    "    \n",
    "    # ---------Crop----------- #\n",
    "    left_margin = (pil_image.width - 224) / 2\n",
    "    bottom_margin = (pil_image.height - 224) / 2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 224\n",
    "    \n",
    "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
    "    \n",
    "    # --------- Convert to np then Normalize ----------- #\n",
    "    np_image = np.array(pil_image) / 255\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    np_image = (np_image -mean) / std\n",
    "    \n",
    "    # --------- Transpose to fit PyTorch Axes ----------#\n",
    "    np_image = np_image.transpose([2, 0, 1])\n",
    "    \n",
    "    return np_image\n",
    "\n",
    "def imshow(pt_image, ax = None, title = None):\n",
    "    '''\n",
    "    Takes in a PyTorch-compatible image with [Ch, H, W],\n",
    "    Convert it back to [H, W, Ch], \n",
    "    Undo the preprocessing,\n",
    "    then display it on a grid\n",
    "    '''\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # --------- Transpose ----------- #\n",
    "    plt_image = pt_image.transpose((1, 2, 0))\n",
    "    \n",
    "    # --------- Undo the preprocessing --------- #\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    plt_image = plt_image * std + mean\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "        \n",
    "    # Image need to be clipped between 0 and 1 or it looks noisy\n",
    "    plt_image = np.clip(plt_image, 0, 1)\n",
    "    \n",
    "    # this imshow is a function defined in the plt module\n",
    "    ax.imshow(plt_image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ab8bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77, 77, 77, ..., 62, 62, 62]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_path = './data/flowers-102/imagelabels.mat'\n",
    "label_arr = scp.loadmat(label_path)['labels']\n",
    "label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712b6d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1020)\n",
      "(1, 1020)\n",
      "(1, 6149)\n"
     ]
    }
   ],
   "source": [
    "split_path = './data/flowers-102/setid.mat'\n",
    "data_splits = scp.loadmat(split_path)\n",
    "train_split = data_splits['trnid']\n",
    "print(train_split.shape)\n",
    "val_split = data_splits['valid']\n",
    "print(val_split.shape)\n",
    "test_split = data_splits['tstid']\n",
    "print(test_split.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e5fd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28fac59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import eval\n",
    "from model import vgg, resnet, mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01f33ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c817dbb3",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b404863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: (0.827939502358107, 1.3834412675731036)\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "PATH = './saved_models/mobilenet_model_weights.pth'\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "model,optimizer,criterion = mobilenet()\n",
    "model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "test_accuracy, _ = eval(test_loader, model, criterion, DEVICE)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e2357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3746e80",
   "metadata": {},
   "source": [
    "### Initialising Few Shot Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb27d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_for_episode(model, criterion, optimizer, support_loader, query_loader, device):\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate through support set\n",
    "    for support_batch in support_loader:\n",
    "        support_inputs, support_targets = support_batch\n",
    "        support_inputs, support_targets = support_inputs.to(device), support_targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        support_outputs = model(support_inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(support_outputs, support_targets)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate on the query set\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for query_batch in query_loader:\n",
    "            query_inputs, query_targets = query_batch\n",
    "            query_inputs, query_targets = query_inputs.to(device), query_targets.to(device)\n",
    "\n",
    "            query_outputs = model(query_inputs)\n",
    "            _, predicted = torch.max(query_outputs, 1)\n",
    "            total_samples += query_targets.size(0)\n",
    "            total_correct += (predicted == query_targets).sum().item()\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec9de6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model_on_test_set(model, test_loader, device):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     total_correct = 0\n",
    "#     total_samples = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             inputs, targets = batch\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "#             # Calculate accuracy for the batch\n",
    "#             correct = (predicted == targets).sum().item()\n",
    "#             total_correct += correct\n",
    "#             total_samples += targets.size(0)\n",
    "    \n",
    "#     accuracy = total_correct / total_samples\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52a60023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for episode 0: 0.0\n",
      "Accuracy for episode 1: 0.0\n",
      "Accuracy for episode 2: 0.0\n",
      "Accuracy for episode 3: 0.0\n",
      "Accuracy for episode 4: 0.0\n",
      "Accuracy for episode 5: 0.2\n",
      "Accuracy for episode 6: 0.0\n",
      "Accuracy for episode 7: 0.0\n",
      "Accuracy for episode 8: 0.0\n",
      "Accuracy for episode 9: 0.0\n",
      "Accuracy for episode 10: 0.0\n",
      "Accuracy for episode 11: 0.0\n",
      "Accuracy for episode 12: 0.0\n",
      "Accuracy for episode 13: 0.0\n",
      "Accuracy for episode 14: 0.0\n",
      "Accuracy for episode 15: 0.0\n",
      "Accuracy for episode 16: 0.0\n",
      "Accuracy for episode 17: 0.0\n",
      "Accuracy for episode 18: 0.0\n",
      "Accuracy for episode 19: 0.0\n",
      "Accuracy for episode 20: 0.2\n",
      "Accuracy for episode 21: 0.0\n",
      "Accuracy for episode 22: 0.0\n",
      "Accuracy for episode 23: 0.0\n",
      "Accuracy for episode 24: 0.0\n",
      "Accuracy for episode 25: 0.0\n",
      "Accuracy for episode 26: 0.0\n",
      "Accuracy for episode 27: 0.1\n",
      "Accuracy for episode 28: 0.2\n",
      "Accuracy for episode 29: 0.0\n",
      "Accuracy for episode 30: 0.0\n",
      "Accuracy for episode 31: 0.2\n",
      "Accuracy for episode 32: 0.0\n",
      "Accuracy for episode 33: 0.0\n",
      "Accuracy for episode 34: 0.3\n",
      "Accuracy for episode 35: 0.0\n",
      "Accuracy for episode 36: 0.1\n",
      "Accuracy for episode 37: 0.1\n",
      "Accuracy for episode 38: 0.0\n",
      "Accuracy for episode 39: 0.0\n",
      "Accuracy for episode 40: 0.1\n",
      "Accuracy for episode 41: 0.0\n",
      "Accuracy for episode 42: 0.0\n",
      "Accuracy for episode 43: 0.0\n",
      "Accuracy for episode 44: 0.3\n",
      "Accuracy for episode 45: 0.0\n",
      "Accuracy for episode 46: 0.0\n",
      "Accuracy for episode 47: 0.1\n",
      "Accuracy for episode 48: 0.1\n",
      "Accuracy for episode 49: 0.0\n",
      "Accuracy for episode 50: 0.0\n",
      "Accuracy for episode 51: 0.2\n",
      "Accuracy for episode 52: 0.0\n",
      "Accuracy for episode 53: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "N = 5  # We initialise 5 Classes to be processed per episode\n",
    "K = 1  # Number of support-set images per class\n",
    "Q = 2  # Number of query images per class\n",
    "total_episodes = 102  # Total number of episodes for training\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_EPOCH = 100\n",
    "model,optimizer,criterion = mobilenet()\n",
    "\n",
    "# Extracting Class Labels\n",
    "\n",
    "class_labels = set()\n",
    "for _, label in train_dataset:\n",
    "    class_labels.add(label)\n",
    "\n",
    "class_labels = list(class_labels) # Converting to list for easier manipulation\n",
    "\n",
    "class_to_idx = {class_label: idx for idx, class_label in enumerate(class_labels)}\n",
    "\n",
    "# Creating the training loop\n",
    "for episode in range(total_episodes):\n",
    "    \n",
    "    sampled_classes = random.sample(class_labels, N)\n",
    "\n",
    "    # Step 2: Sampling support-set and query-set images\n",
    "    support_set = []\n",
    "    query_set = []\n",
    "    for class_name in sampled_classes:\n",
    "        class_indices = [i for i, (_, label) in enumerate(train_dataset) if label == class_to_idx[class_name]]\n",
    "        support_indices = random.sample(class_indices, K)\n",
    "        query_indices = random.sample(class_indices, Q)\n",
    "        \n",
    "        # Organize support-set and query-set images\n",
    "        support_set.extend([train_dataset[i] for i in support_indices])\n",
    "        query_set.extend([train_dataset[i] for i in query_indices])\n",
    "    \n",
    "    support_loader = DataLoader(support_set, batch_size = 256, shuffle=True)\n",
    "    query_loader = DataLoader(query_set, batch_size = 256, shuffle=False)\n",
    "\n",
    "    # Step 3: Training the model for each episode\n",
    "    accuracy = train_model_for_episode(model, criterion, optimizer, support_loader, query_loader, DEVICE) # Using Model, Criterion and Optimiser from nn_setup()\n",
    "    print(f'Accuracy for episode {episode}: {accuracy}')\n",
    "\n",
    "# Step 4: Evaluating the model on the validation dataset\n",
    "# evaluate_model_on_test_set(model, val_loader, DEVICE)\n",
    "val_accuracy, _ = eval(val_loader, model, criterion, DEVICE)\n",
    "print(f'Val Accuracy: {val_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
