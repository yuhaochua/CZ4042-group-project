{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io as scp\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from dataset import train_dataset, test_dataset, val_dataset\n",
    "from torch.utils.data import Dataset  # Import the Dataset class\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.labels = np.array([item[1] for item in dataset])\n",
    "        self.label_to_indices = {label: np.where(self.labels == label)[0] for label in np.unique(self.labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor, anchor_label = self.dataset[idx]\n",
    "        positive_idx = idx\n",
    "\n",
    "        while positive_idx == idx:\n",
    "            positive_idx = random.choice(self.label_to_indices[anchor_label])\n",
    "\n",
    "        negative_label = random.choice(list(self.label_to_indices.keys()))\n",
    "        while negative_label == anchor_label:\n",
    "            negative_label = random.choice(list(self.label_to_indices.keys()))\n",
    "\n",
    "        negative_idx = random.choice(self.label_to_indices[negative_label])\n",
    "\n",
    "        positive, _ = self.dataset[positive_idx]\n",
    "        negative, _ = self.dataset[negative_idx]\n",
    "\n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256  # Adjust the batch size as needed\n",
    "\n",
    "train_triplet_dataset = TripletDataset(train_dataset)\n",
    "train_triplet_loader = DataLoader(train_triplet_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    # Calculate Euclidean distances between the anchor, positive, and negative embeddings\n",
    "    distance_positive = F.pairwise_distance(anchor, positive, p=2)\n",
    "    distance_negative = F.pairwise_distance(anchor, negative, p=2)\n",
    "\n",
    "    # Calculate the triplet loss\n",
    "    loss = torch.clamp(distance_positive - distance_negative + margin, min=0.0)\n",
    "    \n",
    "    # Return the average triplet loss over the batch\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval(dataloader, model, criterion, device):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for idx, (data, target) in enumerate(dataloader):\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, target)\n",
    "#             total_loss += loss.item()\n",
    "#             pred = output.argmax(dim=1)\n",
    "\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item() # compare predicted label to actual label\n",
    "#     return correct / len(dataloader.dataset), total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Triplet Loss: 0.9912272691726685, Val Accuracy: 0.00980392156862745\n",
      "Epoch 2: Triplet Loss: 0.9657003283500671, Val Accuracy: 0.00980392156862745\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Yu_Hao\\Desktop\\Y4S1\\CZ4042 neural networks\\CZ4042 group project\\Triplet_Loss.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Yu_Hao/Desktop/Y4S1/CZ4042%20neural%20networks/CZ4042%20group%20project/Triplet_Loss.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Yu_Hao/Desktop/Y4S1/CZ4042%20neural%20networks/CZ4042%20group%20project/Triplet_Loss.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Yu_Hao/Desktop/Y4S1/CZ4042%20neural%20networks/CZ4042%20group%20project/Triplet_Loss.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mfor\u001b[39;00m anchor, positive, negative \u001b[39min\u001b[39;00m train_triplet_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Yu_Hao/Desktop/Y4S1/CZ4042%20neural%20networks/CZ4042%20group%20project/Triplet_Loss.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Yu_Hao/Desktop/Y4S1/CZ4042%20neural%20networks/CZ4042%20group%20project/Triplet_Loss.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         anchor_emb \u001b[39m=\u001b[39m model(anchor)\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\Yu_Hao\\Desktop\\Y4S1\\CZ4042 neural networks\\CZ4042 group project\\Triplet_Loss.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Yu_Hao/Desktop/Y4S1/CZ4042%20neural%20networks/CZ4042%20group%20project/Triplet_Loss.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     negative_label \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_to_indices\u001b[39m.\u001b[39mkeys()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Yu_Hao/Desktop/Y4S1/CZ4042%20neural%20networks/CZ4042%20group%20project/Triplet_Loss.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m negative_idx \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_to_indices[negative_label])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Yu_Hao/Desktop/Y4S1/CZ4042%20neural%20networks/CZ4042%20group%20project/Triplet_Loss.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m positive, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[positive_idx]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Yu_Hao/Desktop/Y4S1/CZ4042%20neural%20networks/CZ4042%20group%20project/Triplet_Loss.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m negative, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[negative_idx]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Yu_Hao/Desktop/Y4S1/CZ4042%20neural%20networks/CZ4042%20group%20project/Triplet_Loss.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m anchor, positive, negative\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\flowers102.py:84\u001b[0m, in \u001b[0;36mFlowers102.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     81\u001b[0m image \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mopen(image_file)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m---> 84\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform:\n\u001b[0;32m     87\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(label)\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1379\u001b[0m, in \u001b[0;36mRandomRotation.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         fill \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fill]\n\u001b[0;32m   1377\u001b[0m angle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegrees)\n\u001b[1;32m-> 1379\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrotate(img, angle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpand, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcenter, fill)\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1129\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m   1128\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m-> 1129\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mrotate(img, angle\u001b[39m=\u001b[39;49mangle, interpolation\u001b[39m=\u001b[39;49mpil_interpolation, expand\u001b[39m=\u001b[39;49mexpand, center\u001b[39m=\u001b[39;49mcenter, fill\u001b[39m=\u001b[39;49mfill)\n\u001b[0;32m   1131\u001b[0m center_f \u001b[39m=\u001b[39m [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m]\n\u001b[0;32m   1132\u001b[0m \u001b[39mif\u001b[39;00m center \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:314\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be PIL Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    313\u001b[0m opts \u001b[39m=\u001b[39m _parse_fill(fill, img)\n\u001b[1;32m--> 314\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mrotate(angle, interpolation, expand, center, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopts)\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:2265\u001b[0m, in \u001b[0;36mImage.rotate\u001b[1;34m(self, angle, resample, expand, center, translate, fillcolor)\u001b[0m\n\u001b[0;32m   2262\u001b[0m     matrix[\u001b[39m2\u001b[39m], matrix[\u001b[39m5\u001b[39m] \u001b[39m=\u001b[39m transform(\u001b[39m-\u001b[39m(nw \u001b[39m-\u001b[39m w) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m, \u001b[39m-\u001b[39m(nh \u001b[39m-\u001b[39m h) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m, matrix)\n\u001b[0;32m   2263\u001b[0m     w, h \u001b[39m=\u001b[39m nw, nh\n\u001b[1;32m-> 2265\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(\n\u001b[0;32m   2266\u001b[0m     (w, h), Transform\u001b[39m.\u001b[39;49mAFFINE, matrix, resample, fillcolor\u001b[39m=\u001b[39;49mfillcolor\n\u001b[0;32m   2267\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:2620\u001b[0m, in \u001b[0;36mImage.transform\u001b[1;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[0;32m   2617\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2618\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmissing method data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2620\u001b[0m im \u001b[39m=\u001b[39m new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode, size, fillcolor)\n\u001b[0;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpalette:\n\u001b[0;32m   2622\u001b[0m     im\u001b[39m.\u001b[39mpalette \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpalette\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\Yu_Hao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:2845\u001b[0m, in \u001b[0;36mnew\u001b[1;34m(mode, size, color)\u001b[0m\n\u001b[0;32m   2843\u001b[0m     im\u001b[39m.\u001b[39mpalette \u001b[39m=\u001b[39m ImagePalette\u001b[39m.\u001b[39mImagePalette()\n\u001b[0;32m   2844\u001b[0m     color \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpalette\u001b[39m.\u001b[39mgetcolor(color)\n\u001b[1;32m-> 2845\u001b[0m \u001b[39mreturn\u001b[39;00m im\u001b[39m.\u001b[39m_new(core\u001b[39m.\u001b[39;49mfill(mode, size, color))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from common_utils import EarlyStopper\n",
    "from train import train, eval\n",
    "from triplet_loss import TripletLoss\n",
    "from collections import OrderedDict\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "EARLY_STOP_THRESHOLD = 3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "early_stopper = EarlyStopper(patience=3)\n",
    "\n",
    "model,optimizer,criterion = mobilenet()\n",
    "model.classifier = nn.Sequential(OrderedDict([ #mobilenet\n",
    "                        ('dropout1', nn.Dropout(0.5)),\n",
    "                        ('fc2', nn.Linear(1280, 102)),\n",
    "                        ('output', nn.Linear(102, 2))\n",
    "                        ]))\n",
    "criterion = TripletLoss(DEVICE)\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for anchor, positive, negative in train_triplet_loader:\n",
    "        optimizer.zero_grad()\n",
    "        anchor_emb = model(anchor)\n",
    "        positive_emb = model(positive)\n",
    "        negative_emb = model(negative)\n",
    "        loss = triplet_loss(anchor_emb, positive_emb, negative_emb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    accuracy, val_loss = eval(val_loader, model, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch + 1}: Triplet Loss: {loss.item()}, Val Accuracy: {accuracy}\")\n",
    "    if early_stopper.early_stop(accuracy):\n",
    "        print(\"Early Stopping...\")\n",
    "        break\n",
    "\n",
    "# early_stopper = EarlyStopper(patience=EARLY_STOP_THRESHOLD)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# best_acc = 0\n",
    "# early_stop_count = 0\n",
    "# for epoch in range(1, NUM_EPOCHS+1):\n",
    "#     train_loss = train(train_loader, model, criterion, optimizer, DEVICE)\n",
    "#     accuracy, val_loss = eval(val_loader, model, criterion, DEVICE)\n",
    "#     print(f'Epoch {epoch}, Train Loss: {train_loss}, Val Accuracy: {accuracy}, Val Loss: {val_loss}')\n",
    "#     if early_stopper.early_stop(val_loss):\n",
    "#         print(\"Early Stopping...\")\n",
    "#         break\n",
    "#     scheduler.step()\n",
    "# test_accuracy, _ = eval(test_loader, model, criterion, DEVICE)\n",
    "# print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, _ = eval(test_loader, model, criterion, DEVICE)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
