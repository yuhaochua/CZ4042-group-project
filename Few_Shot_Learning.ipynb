{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5155b6ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as scp\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db955532",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                        [0.229, 0.224, 0.225])])\n",
    "\n",
    "testval_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.Flowers102(root='./data', split='train', download=True, transform=train_transform)\n",
    "val_dataset = torchvision.datasets.Flowers102(root='./data', split='val', download=True, transform=testval_transform)\n",
    "test_dataset = torchvision.datasets.Flowers102(root='./data', split='test', download=True, transform=testval_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebc8aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(pil_image):    \n",
    "    # -------- Resize with Aspect Ratio maintained--------- #\n",
    "    # First fixing the short axes\n",
    "    if pil_image.size[0] > pil_image.size[1]:\n",
    "        pil_image.thumbnail((10000000, 256))\n",
    "    else:\n",
    "        pil_image.thumbnail((256, 100000000))\n",
    "    \n",
    "    # ---------Crop----------- #\n",
    "    left_margin = (pil_image.width - 224) / 2\n",
    "    bottom_margin = (pil_image.height - 224) / 2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 224\n",
    "    \n",
    "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
    "    \n",
    "    # --------- Convert to np then Normalize ----------- #\n",
    "    np_image = np.array(pil_image) / 255\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    np_image = (np_image -mean) / std\n",
    "    \n",
    "    # --------- Transpose to fit PyTorch Axes ----------#\n",
    "    np_image = np_image.transpose([2, 0, 1])\n",
    "    \n",
    "    return np_image\n",
    "\n",
    "def imshow(pt_image, ax = None, title = None):\n",
    "    '''\n",
    "    Takes in a PyTorch-compatible image with [Ch, H, W],\n",
    "    Convert it back to [H, W, Ch], \n",
    "    Undo the preprocessing,\n",
    "    then display it on a grid\n",
    "    '''\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # --------- Transpose ----------- #\n",
    "    plt_image = pt_image.transpose((1, 2, 0))\n",
    "    \n",
    "    # --------- Undo the preprocessing --------- #\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    plt_image = plt_image * std + mean\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "        \n",
    "    # Image need to be clipped between 0 and 1 or it looks noisy\n",
    "    plt_image = np.clip(plt_image, 0, 1)\n",
    "    \n",
    "    # this imshow is a function defined in the plt module\n",
    "    ax.imshow(plt_image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64ab8bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77, 77, 77, ..., 62, 62, 62]], dtype=uint8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_path = './data/flowers-102/imagelabels.mat'\n",
    "label_arr = scp.loadmat(label_path)['labels']\n",
    "label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "712b6d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1020)\n",
      "(1, 1020)\n",
      "(1, 6149)\n"
     ]
    }
   ],
   "source": [
    "split_path = './data/flowers-102/setid.mat'\n",
    "data_splits = scp.loadmat(split_path)\n",
    "train_split = data_splits['trnid']\n",
    "print(train_split.shape)\n",
    "val_split = data_splits['valid']\n",
    "print(val_split.shape)\n",
    "test_split = data_splits['tstid']\n",
    "print(test_split.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42e5fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return total_loss/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28fac59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, model, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # compare predicted label to actual label\n",
    "    return correct / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d01f33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_setup(dropout=0.5, hidden_layer1 = 120,lr = 0.001):\n",
    "    \n",
    "    model = models.vgg16(pretrained=True)  \n",
    "        \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(25088, 500)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('dropout1', nn.Dropout(dropout)),\n",
    "                          ('fc2', nn.Linear(500, 102)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "        \n",
    "        model.classifier = classifier\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.classifier.parameters(), lr )\n",
    "        \n",
    "        return model , optimizer ,criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817dbb3",
   "metadata": {},
   "source": [
    "### Training Normal Model Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b404863",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_EPOCH = 100\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# HYPERPARAMS TO TUNE\n",
    "NUM_HIDDEN = 128\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 128\n",
    "EARLY_STOP_THRESHOLD = 3\n",
    "LR = 0.001\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "model,optimizer,criterion = nn_setup()\n",
    "model.to(DEVICE)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "best_acc = 0\n",
    "early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba8e2357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4.567100822925568, Val Accuracy: 0.2803921568627451\n",
      "Epoch 2, Train Loss: 3.4106183648109436, Val Accuracy: 0.47352941176470587\n",
      "Epoch 3, Train Loss: 2.7541665732860565, Val Accuracy: 0.6127450980392157\n",
      "Epoch 4, Train Loss: 2.277555912733078, Val Accuracy: 0.6764705882352942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCH\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(train_loader, model, criterion, optimizer, DEVICE)\n\u001b[0;32m----> 3\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(val_loader, model, DEVICE)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_acc:\n",
      "Cell \u001b[0;32mIn[58], line 8\u001b[0m, in \u001b[0;36meval\u001b[0;34m(dataloader, model, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      6\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m      9\u001b[0m     pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(target\u001b[38;5;241m.\u001b[39mview_as(pred))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# compare predicted label to actual label\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[1;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCH+1):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer, DEVICE)\n",
    "    accuracy = eval(val_loader, model, DEVICE)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Val Accuracy: {accuracy}')\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "    if early_stop_count >= EARLY_STOP_THRESHOLD:\n",
    "        print(\"Early Stopping...\")        \n",
    "        break\n",
    "    scheduler.step()\n",
    "test_accuracy = eval(test_loader, model, DEVICE)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3746e80",
   "metadata": {},
   "source": [
    "### Initialising Few Shot Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8175c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in train_dataset: 102\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(item[1] for item in train_dataset)\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(f\"Number of classes in train_dataset: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d840299",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_classes = 102\n",
    "\n",
    "few_shot_per_class = 5  # We halve the number of Images for each class\n",
    "\n",
    "# Creaing a subset of the training dataset with few-shot images\n",
    "few_shot_train_dataset = []\n",
    "for class_idx in range(total_classes):\n",
    "    # Find the indices of examples in the selected class\n",
    "    indices = [i for i, (_, label) in enumerate(train_dataset) if label == class_idx]\n",
    "    \n",
    "    # Randomly select a few-shot subset of examples\n",
    "    few_shot_indices = random.sample(indices, few_shot_per_class)\n",
    "    \n",
    "    # Add the few-shot examples to the new dataset\n",
    "    few_shot_train_dataset.extend([train_dataset[i] for i in few_shot_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8e3cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for the few-shot training dataset\n",
    "few_shot_train_loader = DataLoader(few_shot_train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a64fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCH+1):\n",
    "    train_loss = train(few_shot_train_loader, model, criterion, optimizer, DEVICE)\n",
    "    accuracy = eval(val_loader, model, DEVICE)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Val Accuracy: {accuracy}')\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "    if early_stop_count >= EARLY_STOP_THRESHOLD:\n",
    "        print(\"Early Stopping...\")        \n",
    "        break\n",
    "    scheduler.step()\n",
    "test_accuracy = eval(test_loader, model, DEVICE)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
